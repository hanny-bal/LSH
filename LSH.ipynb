{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality Sensitive Hashing\n",
    "A Python implementation for the [Introduction to Big Data Algorithms](https://sites.google.com/view/ibda-summer23/home) course at the [University of Salzburg](https://www.plus.ac.at/) in the summer semester 2023. The goal of this notebook is to compare different approaches of finding near-duplicate documents in a list of job announcements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from time import perf_counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.shingling import get_shingles\n",
    "from src.metrics import jaccard_similarity, signature_similarity\n",
    "from src.min_hash import MinHash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We say that two documents are near duplicates if their Jaccard similarity is $\\geq 0.8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s: float = 0.8 # similarity threshold\n",
    "k: int = 10 # length of the shingles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of signature length $h$ and thus also values for $r$ and $b$, we first need to observe how long the shingled documents are. Each job announcement is quite long and can easily go beyond 1000 shingles for shingle length 10. We want to to test different setups. Using the min-hash property $\\mathbb P[s_1[i] = s_2[i]] = \\text{sim}(D_1,D_2)$, I argue for the following values as they represent different probabilities for false positives and false negatives.\n",
    "\n",
    "- $r=5$ and $b=20$ $\\Rightarrow h=100$. \n",
    "    - Then if the similarity of two documents is $0.8$, the probability that they are NOT similar in all bands is $(1-(0.8)^5)^{20} = 0.00035$.\n",
    "    - And if the similarity of two documents is $0.3$, the probability that they identical in at least one band is $1-(1-(0.3)^5)^{20} = 0.0474$.\n",
    "- $r=6$ and $b=10$ $\\Rightarrow h=60$. \n",
    "    - Then if the similarity of two documents is $0.8$, the probability that they are NOT similar in all bands is $(1-(0.8)^6)^{10} = 0.04783$.\n",
    "    - And if the similarity of two documents is $0.3$, the probability that they identical in at least one band is $1-(1-(0.3)^6)^{10} = 0.00726$.\n",
    "- $r=7$ and $b=25$ $\\Rightarrow h=175$. \n",
    "    - Then if the similarity of two documents is $0.8$, the probability that they are NOT similar in all bands is $(1-(0.8)^7)^{25} = 0.00278$.\n",
    "    - And if the similarity of two documents is $0.3$, the probability that they identical in at least one band is $1-(1-(0.3)^7)^{25} = 0.00545$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "setups: dict = {\n",
    "    0: { 'r': 5, 'b': 20, 'h': 100 },\n",
    "    1: { 'r': 6, 'b': 10, 'h': 60 },\n",
    "    2: { 'r': 7, 'b': 25, 'h': 175 },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load the Data\n",
    "First, we need to load the data. To make comparisons, we use the data set `job-ads.txt` located in `data`. I also prepared a larget data set `job-ads-large.txt` where I copied each entry four times to make more meaningful performance comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>movia spa opera da oltre un decennio nel merca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cerchiamo un grafico web designer da inserire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blog di anime, manga e videogames cerca artico...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dps soluzioni informatiche, società specializz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>società operante nell'ambito della comunicazio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  movia spa opera da oltre un decennio nel merca...\n",
       "1  cerchiamo un grafico web designer da inserire ...\n",
       "2  blog di anime, manga e videogames cerca artico...\n",
       "3  dps soluzioni informatiche, società specializz...\n",
       "4  società operante nell'ambito della comunicazio..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs: pd.DataFrame = pd.read_csv('./data/job-ads.txt', header=None)\n",
    "jobs.rename(columns = {0:'text'}, inplace = True)\n",
    "\n",
    "# normally, makes sense to put to lowercase when processing it\n",
    "jobs['text'] = jobs['text'].apply(lambda x: x.lower())\n",
    "\n",
    "# print the head\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the equivalent for the large data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jobs_large: pd.DataFrame = pd.read_csv('./data/job-ads-large.txt', header=None)\n",
    "jobs_large.rename(columns = {0:'text'}, inplace = True)\n",
    "jobs_large['text'] = jobs_large['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Naive Comparison of Shingles\n",
    "The first approach to search for near-duplicates will be a naive comparison of shingles. That is, we shingle each document and compare them directly using the Jaccard similarity. We find near duplicates by finding the near duplicate for each job announcement, i.e. create pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's compute the shingles first\n",
    "naive_shingles: list[set[str]] = jobs['text'].apply(lambda x: get_shingles(document=x, k=k, compressed=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can go ahead and try to get pairs of (near) duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1530/1530 [02:13<00:00, 11.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 133.52640269599942 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# start time\n",
    "t1_naive: float = perf_counter()\n",
    "\n",
    "# set of duplicates - the set makes sure that duplicates like (1,2) and (2,1) don't occur\n",
    "duplicates_naive: set[frozenset[int]] = set()\n",
    "\n",
    "# go through each row\n",
    "for idx, shingle in tqdm(enumerate(naive_shingles), total=len(naive_shingles)):\n",
    "    \n",
    "    # compare the text with each other text\n",
    "    for candidate_idx, candidate_shingle in enumerate(naive_shingles):\n",
    "        sim: float = jaccard_similarity(shingle, candidate_shingle)\n",
    "        if sim >= s and idx != candidate_idx:\n",
    "            duplicates_naive.add(frozenset((idx, candidate_idx)))\n",
    "            \n",
    "# stop time\n",
    "t2_naive: float = perf_counter()\n",
    "elapsed_time_naive: float = t2_naive - t1_naive\n",
    "print(f'Elapsed time: {elapsed_time_naive} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can't be any false positives or false negatives since we compute the true (actual) similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Minhash Comparison\n",
    "We can also try to find documents by comparing their minhash signature in a pairwise manner. In other words, we only try to estimate the document's similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing minhash representation for {'r': 5, 'b': 20, 'h': 100}\n",
      "Preparing minhash representation for {'r': 6, 'b': 10, 'h': 60}\n",
      "Preparing minhash representation for {'r': 7, 'b': 25, 'h': 175}\n"
     ]
    }
   ],
   "source": [
    "compressed_shingles: list[set[int]] = jobs['text'].apply(lambda x: get_shingles(document=x, k=k))\n",
    "\n",
    "# compute signatures for different choices of r and b\n",
    "signatures: dict[list[int]] = {}\n",
    "\n",
    "for key in setups.keys():\n",
    "    print(f'Preparing minhash representation for {setups[key]}')\n",
    "    \n",
    "    # setup a minhash instance\n",
    "    minhash: MinHash = MinHash(K=setups[key]['h'])\n",
    "    \n",
    "    # and compute the signature\n",
    "    signatures[key] = []\n",
    "    for shingle_set in compressed_shingles:\n",
    "        signatures[key].append(minhash.minhash(shingle_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pre-computing the min-hash signatures, we can then make pairwise comparisons of the document signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare a holding data structure for times and statistics\n",
    "times: dict = {}\n",
    "fp: dict = {}\n",
    "fn: dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running minhash similarity search for {'r': 5, 'b': 20, 'h': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1530/1530 [00:12<00:00, 122.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 12.443188043995178 seconds\n",
      "False positives (fp): 12\n",
      "False negatives (fn): 5\n",
      "Running minhash similarity search for {'r': 6, 'b': 10, 'h': 60}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1530/1530 [00:06<00:00, 221.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 6.907350875997508 seconds\n",
      "False positives (fp): 36\n",
      "False negatives (fn): 5\n",
      "Running minhash similarity search for {'r': 7, 'b': 25, 'h': 175}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1530/1530 [00:21<00:00, 72.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 21.212867485999595 seconds\n",
      "False positives (fp): 7\n",
      "False negatives (fn): 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "duplicates = {}\n",
    "for key in setups.keys():\n",
    "    print(f'Running minhash similarity search for {setups[key]}')\n",
    "    \n",
    "    # start time\n",
    "    t1: float = perf_counter()\n",
    "\n",
    "    # set of duplicates - the set makes sure that duplicates like (1,2) and (2,1) don't occur\n",
    "    duplicates[key]: set[frozenset[int]] = set()\n",
    "\n",
    "    # go through each row\n",
    "    for idx, signature in tqdm(enumerate(signatures[key]), total=len(signatures[key])):\n",
    "\n",
    "        # compare the text with each other text\n",
    "        for candidate_idx, candidate_signature in enumerate(signatures[key]):\n",
    "            sim: float = signature_similarity(signature, candidate_signature)\n",
    "            if sim >= s and idx != candidate_idx:\n",
    "                duplicates[key].add(frozenset((idx, candidate_idx)))\n",
    "\n",
    "    # stop time\n",
    "    t2: float = perf_counter()\n",
    "    times[f'minhash_{key}'] = t2 - t1\n",
    "    print(f'Elapsed time: {times[f\"minhash_{key}\"]} seconds')\n",
    "    \n",
    "    # compute false negatives and false positives\n",
    "    fp[f'minhash_{key}'] = duplicates[key].difference(duplicates_naive)\n",
    "    fn[f'minhash_{key}'] = duplicates_naive.difference(duplicates[key])\n",
    "    print(f'False positives (fp): {len(fp[f\"minhash_{key}\"])}')\n",
    "    print(f'False negatives (fn): {len(fn[f\"minhash_{key}\"])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
